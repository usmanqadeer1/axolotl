#!/bin/bash
set -e

echo "=========================================="
echo "Complete Gemma 3 Training Setup"
echo "=========================================="

cd /workspace/axolotl

# ============================================================================
# Step 1: Clear GPU Memory
# ============================================================================
echo ""
echo "Step 1: Clearing GPU memory..."
pkill -9 python || true
sleep 3

python << 'CLEAR_GPU'
import torch
import gc
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("✓ GPU cache cleared")
CLEAR_GPU

# Set memory optimization
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

echo ""
nvidia-smi
echo ""

# ============================================================================
# Step 2: Create Configuration File (Memory Optimized)
# ============================================================================
echo "Step 2: Creating optimized config..."

cat > gemma3-4b-urdu-train.yml << 'CONFIG'
base_model: google/gemma-3-4b-it
model_type: AutoModelForCausalLM
trust_remote_code: true

# Quantization for memory efficiency
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: bfloat16

# Gemma 3 specific
chat_template: gemma3
eot_tokens:
  - <end_of_turn>

datasets:
  - path: large-traversaal/urdu_datasets
    name: urdu_instruct
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content

dataset_prepared_path: prepared_data/urdu_gemma3
val_set_size: 0.01
output_dir: ./outputs/gemma3-4b-urdu

# LoRA settings
adapter: qlora
sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

lora_r: 16
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true

# Training settings - memory optimized
gradient_accumulation_steps: 8
micro_batch_size: 1
num_epochs: 1

optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

bf16: auto
fp16: false
tf32: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# Logging
logging_steps: 1
warmup_ratio: 0.1
weight_decay: 0.0

# Evaluation - skip initial eval to avoid issues
eval_strategy: steps
eval_steps: 100
saves_per_epoch: 1

# Weights & Biases
wandb_project: gemma-3-4b-urdu
wandb_entity: traversaal-research
wandb_watch:
wandb_name:
wandb_log_model:
CONFIG

echo "✓ Config file created"

# ============================================================================
# Step 3: Create Patch for transformers.modelcard
# ============================================================================
echo ""
echo "Step 3: Creating modelcard patch..."

cat > patch_modelcard.py << 'PATCH_MODELCARD'
"""
Patch for transformers.modelcard compatibility
"""
import transformers

if not hasattr(transformers, 'modelcard'):
    class DummyModelCard:
        AUTOGENERATED_TRAINER_COMMENT = ""
    transformers.modelcard = DummyModelCard()
    print("✓ Patched transformers.modelcard")
else:
    print("✓ transformers.modelcard exists")
PATCH_MODELCARD

echo "✓ Modelcard patch created"

# ============================================================================
# Step 4: Create Token Type IDs Patch (Gemma 3 Requirement)
# ============================================================================
echo ""
echo "Step 4: Creating token_type_ids patch..."

cat > gemma3_token_ids_patch.py << 'PATCH_TOKEN_IDS'
"""
Comprehensive token_type_ids patch for Gemma 3
Patches at model level to auto-generate token_type_ids
"""
import torch

print("Applying token_type_ids patches...")

# Patch 1: Gemma3Model.forward (core model)
try:
    from transformers.models.gemma3.modeling_gemma3 import Gemma3Model
    _original_gemma3model_forward = Gemma3Model.forward
    
    def patched_gemma3model_forward(
        self,
        input_ids=None,
        attention_mask=None,
        position_ids=None,
        past_key_values=None,
        inputs_embeds=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        cache_position=None,
        token_type_ids=None,
        **kwargs
    ):
        # Auto-generate token_type_ids if missing
        if token_type_ids is None and input_ids is not None:
            batch_size, seq_len = input_ids.shape
            token_type_ids = torch.zeros(
                (batch_size, seq_len), 
                dtype=torch.long, 
                device=input_ids.device
            )
        
        return _original_gemma3model_forward(
            self,
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
            token_type_ids=token_type_ids,
            **kwargs
        )
    
    Gemma3Model.forward = patched_gemma3model_forward
    print("✓ Patched Gemma3Model.forward")
except Exception as e:
    print(f"! Gemma3Model patch failed: {e}")

# Patch 2: Gemma3ForCausalLM.forward (wrapper model)
try:
    from transformers.models.gemma3.modeling_gemma3 import Gemma3ForCausalLM
    _original_causal_forward = Gemma3ForCausalLM.forward
    
    def patched_causal_forward(
        self,
        input_ids=None,
        attention_mask=None,
        position_ids=None,
        past_key_values=None,
        inputs_embeds=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        cache_position=None,
        token_type_ids=None,
        **kwargs
    ):
        # Auto-generate token_type_ids if missing during training
        if token_type_ids is None and self.training and input_ids is not None:
            batch_size, seq_len = input_ids.shape
            token_type_ids = torch.zeros(
                (batch_size, seq_len), 
                dtype=torch.long, 
                device=input_ids.device
            )
            # Mark assistant tokens based on labels if available
            if labels is not None:
                token_type_ids[labels != -100] = 1
        
        return _original_causal_forward(
            self,
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            labels=labels,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
            token_type_ids=token_type_ids,
            **kwargs
        )
    
    Gemma3ForCausalLM.forward = patched_causal_forward
    print("✓ Patched Gemma3ForCausalLM.forward")
except Exception as e:
    print(f"! Gemma3ForCausalLM patch failed: {e}")

print("=" * 60)
print("Token type IDs patches applied successfully!")
print("=" * 60)
PATCH_TOKEN_IDS

echo "✓ Token IDs patch created"

# ============================================================================
# Step 5: Create Training Script
# ============================================================================
echo ""
echo "Step 5: Creating training script..."

cat > train_gemma3.py << 'TRAIN_SCRIPT'
#!/usr/bin/env python3
"""
Gemma 3 Training Script with All Patches
"""
import sys
import os

print("\n" + "=" * 70)
print(" " * 20 + "GEMMA 3 TRAINING")
print("=" * 70)
print("\nApplying patches...\n")

# Apply all patches before importing axolotl
exec(open('/workspace/axolotl/patch_modelcard.py').read())
exec(open('/workspace/axolotl/gemma3_token_ids_patch.py').read())

print("\n" + "=" * 70)
print("All patches applied successfully! Starting training...")
print("=" * 70 + "\n")

# Now import and run Axolotl
from axolotl.cli.train import do_cli
import fire

if __name__ == "__main__":
    fire.Fire(do_cli)
TRAIN_SCRIPT

chmod +x train_gemma3.py
echo "✓ Training script created"

# ============================================================================
# Step 6: Test Patches
# ============================================================================
echo ""
echo "Step 6: Testing patches..."

python << 'TEST_PATCHES'
import sys
sys.path.insert(0, '/workspace/axolotl')

print("\n" + "=" * 60)
print("Testing Patches")
print("=" * 60 + "\n")

# Test modelcard patch
exec(open('/workspace/axolotl/patch_modelcard.py').read())
import transformers
assert hasattr(transformers, 'modelcard'), "Modelcard patch failed!"
print("✅ Modelcard patch test passed\n")

# Test token_type_ids patch
exec(open('/workspace/axolotl/gemma3_token_ids_patch.py').read())

# Try to import Gemma3 models
try:
    from transformers.models.gemma3.modeling_gemma3 import Gemma3Model, Gemma3ForCausalLM
    print("✅ Gemma3 models imported successfully\n")
except Exception as e:
    print(f"⚠️  Warning: Could not import Gemma3 models: {e}\n")

print("=" * 60)
print("All patch tests completed!")
print("=" * 60 + "\n")
TEST_PATCHES

if [ $? -ne 0 ]; then
    echo "❌ Patch tests failed. Exiting."
    exit 1
fi

# ============================================================================
# Step 7: Run Preprocessing
# ============================================================================
echo ""
echo "Step 7: Running preprocessing..."
echo "=========================================="

accelerate launch -m axolotl.cli.preprocess gemma3-4b-urdu-train.yml

if [ $? -ne 0 ]; then
    echo "❌ Preprocessing failed. Exiting."
    exit 1
fi

echo "✓ Preprocessing completed successfully"

# ============================================================================
# Step 8: Run Training
# ============================================================================
echo ""
echo "Step 8: Starting training with all patches..."
echo "=========================================="
echo ""

accelerate launch train_gemma3.py gemma3-4b-urdu-train.yml

if [ $? -eq 0 ]; then
    echo ""
    echo "=========================================="
    echo "✅ TRAINING COMPLETED SUCCESSFULLY!"
    echo "=========================================="
    echo ""
    echo "Model saved to: ./outputs/gemma3-4b-urdu"
    echo ""
    echo "To use your fine-tuned model:"
    echo "  from peft import PeftModel"
    echo "  from transformers import AutoModelForCausalLM"
    echo "  base_model = AutoModelForCausalLM.from_pretrained('google/gemma-3-4b-it')"
    echo "  model = PeftModel.from_pretrained(base_model, './outputs/gemma3-4b-urdu')"
    echo ""
else
    echo ""
    echo "=========================================="
    echo "❌ TRAINING FAILED"
    echo "=========================================="
    echo ""
    echo "Check the error messages above for details."
    echo "Common issues:"
    echo "  1. GPU out of memory - reduce micro_batch_size or sequence_len"
    echo "  2. Dataset issues - check your dataset path and format"
    echo "  3. HuggingFace authentication - run 'huggingface-cli login'"
    echo ""
    exit 1
fi

